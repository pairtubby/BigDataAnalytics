{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UjlYUoo3nK-9"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext,SQLContext  \n",
        "from pyspark.sql import SparkSession   \n",
        "from pyspark.ml.feature import Word2Vec,CountVectorizer  \n",
        "from pyspark.ml.clustering import LDA, LDAModel  \n",
        "from pyspark.sql.functions import col, udf  \n",
        "from pyspark.sql.types import IntegerType,ArrayType,StringType  \n",
        "import pylab as pl  \n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y4jjf6wWnU3D"
      },
      "outputs": [],
      "source": [
        "def to_word(termIndices):\n",
        "  words = []  \n",
        "  for termID in termIndices:\n",
        "    words.append(vocab_broadcast.value[termID])      \n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RbEMpBH_nrPy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/10/24 02:14:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/10/24 02:14:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+--------------------+---+--------------------+\n",
            "|               words| id|         token_words|\n",
            "+--------------------+---+--------------------+\n",
            "|I absolutely ADOR...|  0|[i, absolutely, a...|\n",
            "|Java Vs Python Fo...|  1|[java, vs, python...|\n",
            "|voulu un grec pui...|  2|[voulu, un, grec,...|\n",
            "|Pareil Il pris de...|  3|[pareil, il, pris...|\n",
            "|Music Academy Blo...|  4|[music, academy, ...|\n",
            "|Tarps, tents, and...|  5|[tarps,, tents,, ...|\n",
            "|voulu un grec pui...|  6|[voulu, un, grec,...|\n",
            "|We drive efficien...|  7|[we, drive, effic...|\n",
            "|Check out my Gig ...|  8|[check, out, my, ...|\n",
            "|Hey, nice bones y...|  9|[hey,, nice, bone...|\n",
            "|lembro como sofri...| 10|[lembro, como, so...|\n",
            "|WHO WITH A DEEP T...| 11|[who, with, a, de...|\n",
            "|@Tina69911364 @As...| 12|[@tina69911364, @...|\n",
            "|alguem cria um ap...| 13|[alguem, cria, um...|\n",
            "|@Neptvn08 Comment...| 14|[@neptvn08, comme...|\n",
            "|une dinguerie de ...| 15|[une, dinguerie, ...|\n",
            "|Y a une grosse mo...| 16|[y, a, une, gross...|\n",
            "|Je te cache pas q...| 17|[je, te, cache, p...|\n",
            "|@JAPANFESS setauk...| 18|[@japanfess, seta...|\n",
            "|Femme recherchant...| 19|[femme, rechercha...|\n",
            "+--------------------+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Load your document dataframe here\n",
        "#================your code here==================\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql.functions import monotonically_increasing_id \n",
        "\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "spark_df= spark.read.format(\"csv\").option(\"header\",'false').load(\"stream_data.csv\")\n",
        "print(type(spark_df))\n",
        "spark_df = spark_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
        "spark_df = spark_df.withColumnRenamed('_c0','words')\n",
        "tokenizer = Tokenizer(inputCol='words',outputCol='token_words')\n",
        "spark_df = tokenizer.transform(spark_df)\n",
        "\n",
        "\n",
        "\n",
        "#==================================================\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M16Wh6YhoDoH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|               words| id|         token_words|        raw_features|            features|\n",
            "+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|I absolutely ADOR...|  0|[i, absolutely, a...|(145,[1,8,13,20,5...|(145,[1,8,13,20,5...|\n",
            "|Java Vs Python Fo...|  1|[java, vs, python...|(145,[19,74],[1.0...|(145,[19,74],[2.5...|\n",
            "|voulu un grec pui...|  2|[voulu, un, grec,...|(145,[7,15,16,54,...|(145,[7,15,16,54,...|\n",
            "|Pareil Il pris de...|  3|[pareil, il, pris...|(145,[2,11,15,25,...|(145,[2,11,15,25,...|\n",
            "|Music Academy Blo...|  4|[music, academy, ...|(145,[1,3,4,34,12...|(145,[1,3,4,34,12...|\n",
            "|Tarps, tents, and...|  5|[tarps,, tents,, ...|(145,[3,5,20,111]...|(145,[3,5,20,111]...|\n",
            "|voulu un grec pui...|  6|[voulu, un, grec,...|(145,[7,15,16,54,...|(145,[7,15,16,54,...|\n",
            "|We drive efficien...|  7|[we, drive, effic...|(145,[4,70],[1.0,...|(145,[4,70],[1.83...|\n",
            "|Check out my Gig ...|  8|[check, out, my, ...|(145,[13,17,31,56...|(145,[13,17,31,56...|\n",
            "|Hey, nice bones y...|  9|[hey,, nice, bone...|(145,[0,10,21,38,...|(145,[0,10,21,38,...|\n",
            "|lembro como sofri...| 10|[lembro, como, so...|(145,[30,42,57,64...|(145,[30,42,57,64...|\n",
            "|WHO WITH A DEEP T...| 11|[who, with, a, de...|(145,[0,1,5,6,13,...|(145,[0,1,5,6,13,...|\n",
            "|@Tina69911364 @As...| 12|[@tina69911364, @...|(145,[0,3,63],[2....|(145,[0,3,63],[2....|\n",
            "|alguem cria um ap...| 13|[alguem, cria, um...|(145,[10,42,113],...|(145,[10,42,113],...|\n",
            "|@Neptvn08 Comment...| 14|[@neptvn08, comme...|(145,[2,9],[3.0,1...|(145,[2,9],[4.612...|\n",
            "|une dinguerie de ...| 15|[une, dinguerie, ...|(145,[2,22,27,29,...|(145,[2,22,27,29,...|\n",
            "|Y a une grosse mo...| 16|[y, a, une, gross...|(145,[1,22,23,46,...|(145,[1,22,23,46,...|\n",
            "|Je te cache pas q...| 17|[je, te, cache, p...|(145,[2,9,11,15,1...|(145,[2,9,11,15,1...|\n",
            "|@JAPANFESS setauk...| 18|[@japanfess, seta...|(145,[3,7],[1.0,1...|(145,[3,7],[1.586...|\n",
            "|Femme recherchant...| 19|[femme, rechercha...|(145,[14,22,25,44...|(145,[14,22,25,44...|\n",
            "+--------------------+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#CountVectorizer\n",
        "#================your code here==================\n",
        "#TF\n",
        "cv = CountVectorizer(inputCol='token_words',outputCol='raw_features',vocabSize=5000, minDF=10.0)\n",
        "\n",
        "model = cv.fit(spark_df)\n",
        "result = model.transform(spark_df)\n",
        "\n",
        "#IDF\n",
        "idf = IDF(inputCol=\"raw_features\",outputCol=\"features\")\n",
        "idfModel = idf.fit(result)\n",
        "result_tfidf = idf.fit(result)\n",
        "result_tfidf = idfModel.transform(result)\n",
        "#result_tfidf.select('token_words','features').show(truncate=True)\n",
        "result_tfidf.show()\n",
        "cvResult = result_tfidf[[\"id\",\"features\"]]\n",
        "#=================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Y5pLaBZolAq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#train LDA model, cluster the documents into 10 topics \n",
        "#================your code here==================\n",
        "\n",
        "num_topics = 10\n",
        "max_iterations = 100\n",
        "lda = LDA(k=num_topics, seed=1, optimizer=\"em\")\n",
        "lda.setMaxIter(max_iterations)\n",
        "lda.getMaxIter()\n",
        "ldaModel = lda.fit(cvResult)\n",
        "\n",
        "\n",
        "#=================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|topic|         termIndices|         termWeights|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|[5, 6, 21, 47, 20...|[0.13557348228467...|\n",
            "|    1|[9, 23, 25, 32, 4...|[0.16107238029942...|\n",
            "|    2|[22, 27, 29, 40, ...|[0.10496751333839...|\n",
            "|    3|[16, 15, 7, 36, 4...|[0.14013980720935...|\n",
            "|    4|[11, 2, 18, 37, 3...|[0.15383337414683...|\n",
            "|    5|[3, 17, 19, 28, 3...|[0.14737961326364...|\n",
            "|    6|[13, 0, 34, 38, 4...|[0.11986345639154...|\n",
            "|    7|[10, 12, 30, 33, ...|[0.15137624573375...|\n",
            "|    8|[4, 8, 0, 24, 56,...|[0.15226462355724...|\n",
            "|    9|[1, 14, 26, 7, 61...|[0.26968790564013...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DenseMatrix(145, 10, [0.7447, 0.4882, 0.0819, 0.5967, 1.3057, 208.6201, 202.3025, 0.0691, ..., 0.0862, 0.1165, 0.1009, 0.0161, 0.0161, 0.0441, 0.0161, 0.1444], 0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ldaModel.describeTopics().show()\n",
        "ldaModel.topicsMatrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ovzUq8JPow3S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/10/24 02:15:40 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
            "22/10/24 02:15:40 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|topicDistribution                                                                                                                                                                                        |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[0.08388544767655748,0.06655225572664249,0.10629971625348385,0.06655293172997798,0.066551504440452,0.07930907079684008,0.2759582555444431,0.06655557580314661,0.10791012775426025,0.08042511427419628]   |\n",
            "|[0.09078806780054116,0.09078545354363239,0.09078544698560204,0.0907854568391819,0.09078544198558337,0.12947520285191294,0.09079084547153767,0.09078546870651381,0.14423236581353716,0.0907862500019576]  |\n",
            "|[0.07029354340813784,0.07029721433046315,0.07029383802705869,0.36480654163193016,0.07034032992459721,0.070293544316452,0.07029354341166148,0.07029354437134601,0.07029354358426844,0.07279435699408496]  |\n",
            "|[0.06795709016653102,0.10226332252021851,0.0693383906975269,0.16860073558432132,0.21059351769317117,0.06795728225985108,0.06795695815312398,0.06795753627017874,0.06795694058882502,0.10941822606625229] |\n",
            "|[0.08236054244512703,0.08229798618862011,0.08229623393267234,0.0822995625882475,0.08229609263294752,0.10125661234311434,0.12479163639876746,0.0834631529383521,0.1621230274682347,0.11681515306391671]   |\n",
            "|[0.1339106343157941,0.08565542731101647,0.08565521328794747,0.08565599107993624,0.08565505342938662,0.11896957560040508,0.14532492187739943,0.08762733476729519,0.0858641942792571,0.08568165405156242]  |\n",
            "|[0.07029354340813784,0.07029721433046315,0.07029383802705869,0.36480654163193016,0.07034032992459721,0.070293544316452,0.07029354341166148,0.07029354437134601,0.07029354358426844,0.07279435699408496]  |\n",
            "|[0.09184271927269967,0.09178689517242347,0.09178689489228205,0.09178726711480413,0.09178687289975007,0.09181037823736582,0.09180677378267263,0.09314312201524831,0.17245383579907003,0.09179524081368384]|\n",
            "|[0.13200469277758117,0.08018933985101759,0.08018427347562157,0.08019091698813323,0.080184264864693,0.15697455716784625,0.10548780859584704,0.08018415604651892,0.1243914209392168,0.08020856929352455]   |\n",
            "|[0.11407357631766032,0.07787205499754013,0.07786971063602485,0.07787461554319663,0.13034026201328203,0.07832397701172128,0.12639245334789,0.10349096008762998,0.08543600125164903,0.1283263887934058]    |\n",
            "|[0.07488972493657531,0.11173984628220064,0.07486597410822587,0.07508969223304697,0.07488155922838019,0.11875989488524755,0.07486524426324916,0.16079640153949512,0.15921110801914323,0.07490055450443589]|\n",
            "|[0.1434086774298703,0.06934282816233509,0.06934179977410604,0.0693436671621908,0.06934169080670387,0.17486848104272137,0.1700111885748002,0.07084899640679283,0.0798877882912853,0.08360488234919435]    |\n",
            "|[0.0884962759956522,0.08845528441876467,0.0884551043681699,0.0884560142471349,0.08845490922696533,0.16228200162721676,0.1080667775375748,0.08899073120240532,0.10986478015005949,0.08847812122605649]    |\n",
            "|[0.08688616401549744,0.12963774235650308,0.08688667157887128,0.0871468192525803,0.08690534420450864,0.086886329059311,0.08688613896992002,0.17496570212937207,0.08688685983634492,0.08691222859709133]   |\n",
            "|[0.09000468481668904,0.12638788073489707,0.10084113011453816,0.09106510600971367,0.1375001372561042,0.09000473780773761,0.09000468514183496,0.0900094638606882,0.09000469100924095,0.09417748324855618]  |\n",
            "|[0.0510903294642534,0.05148059420955281,0.5365924025418154,0.051179530312938516,0.053855070044350785,0.05109033207661635,0.05109032977602289,0.0510904581364968,0.05109032991196872,0.05144062352598422] |\n",
            "|[0.07752392545622727,0.210405205762641,0.11207209737412042,0.11837723280112342,0.07752950364866226,0.0777099952018377,0.07754123202019834,0.07752408299197901,0.07752132174977619,0.09379540299343442]   |\n",
            "|[0.08085049630457558,0.15704140961992208,0.08323936938481115,0.11326982034607656,0.16030378699356493,0.08085051853900924,0.08085049642008908,0.0808529690440176,0.08085049823911732,0.08189063510881651] |\n",
            "|[0.0943015382883105,0.09429865630789779,0.09428618501734513,0.11473345530003992,0.09435461417372328,0.11671276419645897,0.09649847806424237,0.0942859388212907,0.09452587655123765,0.10600249327945356]  |\n",
            "|[0.08056347349492371,0.16686573219487771,0.15719162740530482,0.0805833442129211,0.08057571253919604,0.08056347362813103,0.08056347349524284,0.08056347381795845,0.08056347350474713,0.11196621570669714] |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "transformed = ldaModel.transform(cvResult).select(\"topicDistribution\")  \n",
        "#show the weight of every topic Distribution \n",
        "transformed.show(truncate=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tz6D0Tllo5bs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ll:  -86694.88478220982\n",
            "lp:  6.524969381452884\n"
          ]
        }
      ],
      "source": [
        "#The higher ll is, the lower lp is, the better model is.\n",
        "ll = ldaModel.logLikelihood(cvResult)  \n",
        "lp = ldaModel.logPerplexity(cvResult)\n",
        "print(\"ll: \", ll)\n",
        "print(\"lp: \", lp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MQ_Ukzz4sS69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learned topics (as distributions over vocab of 145 words):\n",
            "DenseMatrix([[7.44650309e-01, 1.54204742e-01, 1.57275957e-01, ...,\n",
            "              5.36361818e+00, 1.68619487e+02, 3.84673984e-01],\n",
            "             [4.88228759e-01, 2.85729202e-01, 1.85765381e-01, ...,\n",
            "              4.65727450e-01, 3.41844324e-01, 2.94660508e+02],\n",
            "             [8.18953639e-02, 2.69081593e+01, 5.70656668e+01, ...,\n",
            "              2.24942295e-01, 1.02318195e-01, 2.01310157e+01],\n",
            "             ...,\n",
            "             [2.04559343e-02, 4.54045489e-02, 3.94413044e-02, ...,\n",
            "              3.10506245e-02, 2.22042280e-02, 4.40750790e-02],\n",
            "             [1.34201901e-02, 1.34696789e-02, 1.36809182e-02, ...,\n",
            "              4.11737870e+01, 1.70465397e-02, 1.60909869e-02],\n",
            "             [1.04869259e-01, 6.84187746e-02, 7.00306313e-02, ...,\n",
            "              1.20872916e-01, 1.91578228e-01, 1.44446387e-01]])\n"
          ]
        }
      ],
      "source": [
        "# Output topics. Each is a distribution over words (matching word count vectors)\n",
        "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())+ \" words):\")\n",
        "topics = ldaModel.topicsMatrix()\n",
        "print(topics)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "6893_HW2PartII_LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
